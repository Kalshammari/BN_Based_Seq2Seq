# -*- coding: utf-8 -*-
"""LSTM_seq2seq_forecasting_MFP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pUOeuMrut94SRc5-5cCM7MjzsoFopcgx
"""

import numpy as np
import pandas as pd
import networkx as nx
import pickle

#Import The Solar Flare Data Set Files
#Reading pickle files
def load(file_name):
    with open(file_name, 'rb') as fp:
        obj = pickle.load(fp)
    return obj


Sampled_inputs=load("mvts_1540_icmla_21.pck")

Sampled_labels=load("labels_1540_4classes_icmla_21.pck") 

temp=Sampled_inputs[0]
print(temp)
df = pd.DataFrame(temp)
trainData=Sampled_inputs
trainLabel=Sampled_labels
print("trainData.shape: ",trainData.shape)
print("trainLebel.shape: ",trainLabel.shape)

df = pd.DataFrame(trainData[0])
df

#standardization/z normalization of the univaraite time series
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
npArrays=[]
for l in range(0, len(trainData)):
  trainData_std = sc.fit_transform(trainData[l])
  #trainData_std = trainData_std.astype(np.float64)
  #print(type(trainData_std[0][0]))
  npArrays.append(trainData_std)

print(type(npArrays))
arr = np.asarray(npArrays)
print(type(arr))
trainData=arr
print("trainData.shape: ",trainData.shape)
print(type(trainData))
print("trainLebel.shape: ",trainLabel.shape)
print(type(trainLabel))

df = pd.DataFrame(trainData[0])
df

print(trainData)

#Transposing trainData to shape:(1540, 60, 33)
trainDatatemp=np.empty([1540,60, 33])
n=len(trainData)
for l in range(0, n):
  temp=trainData[l]
  temp=temp.T
  trainDatatemp[l,:,:]=temp
  

trainData=trainDatatemp
print("Transposing trainData shape: ",trainData.shape)

#Transposing trainData to shape:(1540, 60, 33)
trainDatat1=np.empty([1540,60, 25])
n=len(trainData)
for l in range(0, n):
  temp=trainData[l,:,0:25]
  trainDatat1[l,:,:]=temp
  

trainData=trainDatat1
print("Transposing trainData shape: ",trainData.shape)

# Stratify the data
from sklearn.model_selection import train_test_split
mvts_1540=trainData
labels_1540=trainLabel
#testing with six different random seeds (random_state = 0, 1, 2, 3, 4, and 5)
X_train, X_test, y_train, y_test = train_test_split(mvts_1540, labels_1540, test_size=0.2, random_state=0, stratify=labels_1540)
print("X_train.shape y_train.shape y_test.shape ",X_train.shape, y_train.shape)
print("X_test.shape y_test.shape ",X_test.shape, y_test.shape)

#check percentage of examples
print("X_train shape: ", X_train.shape)
print("y_train shape: ", y_train.shape)
print("X_test shape: ", X_test.shape)
print("y_test shape: ", y_test.shape)
unique_y_train, counts_y_train = np.unique(y_train, return_counts=True)
y_train_stats = dict(zip(unique_y_train, counts_y_train))
print("y_train_counts")
print(y_train_stats)
#270/(269+269+270+270) = 0.25
unique_y_test, counts_y_test = np.unique(y_test, return_counts=True)
y_test_stats = dict(zip(unique_y_test, counts_y_test))
print("y_test_counts")
print(y_test_stats)
#116/(116+116+115+115) = 0.25

print(y_train[0])

#Partition trainData to shape X:(1078, 40, 33), Y:(1078, 20, 33)
trainData=X_train
trainDatatempx=np.empty([1078,40, 25])
trainDatatempy=np.empty([1078,20, 25])
n=len(trainData)
print(n)
for l in range(0, n):
  trainDatatempx=trainData[:,0:40,:] #trainDatatempx[l,:,:]
  trainDatatempy=trainData[:,40:60,:]

X_train=trainDatatempx
y_train=trainDatatempy
print("trainData X_train shape: ",X_train.shape) 
print("trainData y_train shape: ",y_train.shape)

df = pd.DataFrame(X_train[0])
df

f = pd.DataFrame(y_train[0])
f

#Partition testData to shape X:(462, 40, 33), Y:(462, 20, 33)
testData=X_test
trainDatatempx=np.empty([462,40, 33])
trainDatatempy=np.empty([462,20, 33])
n=len(testData)
print(n)
for l in range(0, n):
  trainDatatempx=testData[:,0:40,:]
  trainDatatempy=testData[:,40:60,:]

X_test=trainDatatempx
y_test=trainDatatempy
print("testData X_train shape: ",X_test.shape) 
print("testData y_test shape: ",y_test.shape)

"""# **Model 1: Building Seq2Seq LSTM in Keras for Time Series Forecasting**"""

import random
import numpy as np
import matplotlib.pyplot as plt

import pickle as pkl
import keras
from keras.models import Sequential, Model, load_model
from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Input, BatchNormalization, multiply, concatenate, Flatten, Activation, dot
#from keras.optimizers import Adam
from keras.optimizers import adam_v2 
#from keras.utils import plot_model
from keras.utils.vis_utils import plot_model
from keras.callbacks import EarlyStopping
import pydot as pyd
from keras.utils.vis_utils import plot_model, model_to_dot
keras.utils.vis_utils.pydot = pyd

#Setting the number of Hidden Layers
n_hidden = 10

X_train.shape[0]

# Setting the input and output layer of the LSTM model
input_train = Input(shape=(X_train.shape[1], X_train.shape[2]))
output_train = Input(shape=(y_train.shape[1], y_train.shape[2]))
print(input_train)
print(output_train)

#The encoder LSTM
encoder_last_h1, encoder_last_h2, encoder_last_c = LSTM(
 n_hidden, activation='elu', dropout=0.2, recurrent_dropout=0.2, 
 return_sequences=False, return_state=True)(input_train)
print(encoder_last_h1)
print(encoder_last_h2)
print(encoder_last_c)

#Batch normalisation is added to avoid gradient explosion caused by the activation function ELU in the encoder
encoder_last_h1 = BatchNormalization(momentum=0.6)(encoder_last_h1)
encoder_last_c = BatchNormalization(momentum=0.6)(encoder_last_c)

#making 20 copies of the last hidden state of encoder and use them as input to the decoder
decoder = RepeatVector(output_train.shape[1])(encoder_last_h1)
decoder = LSTM(n_hidden, activation='elu', dropout=0.2, recurrent_dropout=0.2, return_state=False, return_sequences=True)(
    decoder, initial_state=[encoder_last_h1, encoder_last_c])
print(decoder)

#The last cell state and the last hidden state of the encoder are also used as the initial states of decoder.
out = TimeDistributed(Dense(y_train.shape[2]))(decoder)
print(out)

#Putting everything into the model, and compile it. Using MSE as the loss function and MAE as the evaluation metric 
#Setting clipnorm=1 for Adam optimiser
#This is to normalise the gradient, so as to avoid gradient explosion during back propagation
model = Model(inputs=input_train, outputs=out)
opt = adam_v2.Adam(lr=0.01, clipnorm=1)
model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mae'])
model.summary()

# Plot the model

plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

# Training The Model
from keras.callbacks import ModelCheckpoint, EarlyStopping
epc = 20
es = EarlyStopping(monitor='val_loss', mode='min', patience=50)
history = model.fit(X_train, y_train, validation_split=0.2, epochs=epc, verbose=1, callbacks=[es], batch_size=20)
train_mae = history.history['mae']
valid_mae = history.history['val_mae']
#model.save('model11_forecasting_seq2seq.txt')

import math
print("train mae: ",np.mean(history.history['mae']))
print("validation mae: ",np.mean(history.history['val_mae']))
print("train mse: ",np.mean(history.history['loss']))
print("validation mse: ",np.mean(history.history['val_loss']))
print("train RMSE: ",math.sqrt(np.mean(history.history['loss'])))
print("validation RMSE: ",math.sqrt(np.mean(history.history['val_loss'])))

print(history.history.keys())

# Get training and test loss histories
training_loss = history.history['loss']
test_loss = history.history['val_loss']

# Create count of the number of epochs
epoch_count = range(1, len(training_loss) + 1)

# Visualize loss history
plt.plot(epoch_count, training_loss, 'r--')
plt.plot(epoch_count, test_loss, 'b-')
plt.legend(['Training Loss', 'Validation Loss'])
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show();

plt.plot(train_mae, label='train mae'), 
plt.plot(valid_mae, label='validation mae')
plt.ylabel('mae')
plt.xlabel('epoch')
plt.title('train vs. validation accuracy (mae)')
plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=False, ncol=2)
plt.show()

"""**Prediction:The model prediction as well as the true values are unnormalised**"""

#train_pred= model.predict(X_train)
test_pred= model.predict(X_test)
train_true_detrend = y_train
#print(train_pred.shape, train_true_detrend.shape)
test_true = y_test
print(test_pred.shape)
print(test_true.shape)

FArrays=[]
y_pred=test_pred
print(len(y_test))
print(len(y_pred))
print(y_test.shape)
print(y_pred.shape)
for l in range(0, len(y_test)):
   r=np.linalg.norm(y_test[l]-y_pred[l], 'fro')
   print(r)
   FArrays.append(r)   
print("FArrays :",FArrays)
F=np.sum(FArrays)  
print("Frobenius norm :", F/len(test_pred))

r1=np.linalg.norm(y_test[40], 'fro')
r2=np.linalg.norm(y_pred[40], 'fro')
print("y_test[40]",r1)
print("y_pred[40]",r2)
print("y_test[40] Minus y_pred[40]",r1-r2)
print(np.linalg.norm(y_test[40]- y_pred[40], 'fro'))

r3=np.linalg.norm(y_test[300], 'fro')
r4=np.linalg.norm(y_pred[300], 'fro')
print("y_test[300]",r3)
print("y_pred[300]",r4)
print("y_test[300] Minus y_pred[300]",r3-r4)
print(np.linalg.norm(y_test[300]-y_pred[300], 'fro'))

r5=np.linalg.norm(y_test[5], 'fro')
r6=np.linalg.norm(y_pred[5], 'fro')
print("y_test[5]",r5)
print("y_pred[5]",r6)
print("y_test[5] Minus y_pred[5]",r5-r6)
print(np.linalg.norm(y_test[5]-y_pred[5], 'fro'))

print("y_test[0]")
df = pd.DataFrame(y_test[0])
df

print("test_Pred[0]")
df = pd.DataFrame(test_pred[0])
df

print(np.linalg.norm(y_test[250]-test_pred[250], 'fro'))

#print("train_Pred:",train_pred.shape)
#print("train_true:",y_train.shape)
print("test_Pred:",test_pred.shape)
print("test_true:",y_test.shape)

"""**Printing mean_absolute_error, mean_squared_error and train root mean square error for the train and test predictions**


"""

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error


te=[]
tb=[]
for l in range(0, len(test_pred)):
  t=mean_absolute_error(y_test[l], test_pred[l])
  td=mean_squared_error(y_test[l], test_pred[l])
  te.append(t)
  tb.append(td)
  #print(t)
test_mae= np.sum(te)/len(test_pred)
test_mse= np.sum(td)/len(test_pred)
RMSEtest = math.sqrt(test_mse)

print("test_mae",test_mae)
print("test_mse",test_mse)
print("Test Root Mean Square Error:",RMSEtest)